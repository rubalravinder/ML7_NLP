{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing : Classic to Deep Methods for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-Of-Word and TF-IDF:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "Recurrent Neural Networks (RNNs):\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "Long Short Term Memory networks (LSTMs):\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Word embeddings:\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TOFILL\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/d5learner-15/.local/lib/python3.8/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /home/d5learner-15/.local/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/d5learner-15/.local/lib/python3.8/site-packages (from nltk) (4.61.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/d5learner-15/.local/lib/python3.8/site-packages (from nltk) (2021.10.8)\n",
      "Requirement already satisfied: joblib in /home/d5learner-15/.local/lib/python3.8/site-packages (from nltk) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/d5learner-15/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/d5learner-15/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to tackle the sentiment analysis problem, a *text classification* problem. The idea is pretty simple : we want to automatically predict whether a text expresses positive or negative sentiments. To do so we will use the IMDB dataset, that contains 50000 movie reviews from the www.imdb.com website, and their corresponding sentiment : positive or negative. It is thus a binary classification problem, where we want to predict a binary target $y \\in \\{0,1\\}$. We will go through different ways of encoding a text in a vectorial form $x \\in \\mathbb{R}^d$, as well as different classification models, from classic ways to modern deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore a bit the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and print the dataset\n",
    "imdb_dataset_original=pd.read_csv('../data/IMDB Dataset.csv')\n",
    "imdb_dataset = imdb_dataset_original.copy()\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "#Print first review:\n",
    "print(imdb_dataset[\"review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the two classes size\n",
    "imdb_dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see the text is quite messy, and before encoding our text into features, we are going to go through different preprocessing steps in order to clean it:\n",
    "* Removing the HTML tags.\n",
    "* Removing other special characters : this means all non alphanumeric characters, including punctuation.\n",
    "* Lowercase the text.\n",
    "* Tokenization : split a text as a list of words now called tokens.\n",
    "* Stemming : removing all the suffixes from conjugation, plural, ... In order to bring a word back to its root form. For example.\n",
    "* Removing stopwords : words like 'to', 'a', 'the', ... are called stopwords, we remove them as they are too frequent words and generally just add noise.\n",
    "\n",
    "Fill the following functions to perform each of these steps. You are free to use the libraries of your choice to do so. Try to not reinvent the wheel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to clean from html tags\n",
    "    Output: str : The same string with html tags removed\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    soup = BeautifulSoup(text, \"html.parser\") # on crée une instance de l'objet BeautifulSoup\n",
    "    stripped_text = soup.get_text(separator=\" \") # on veut le texte en remplacant les html tags par des espaces\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me. The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word. It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away. I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_html_tags) # on applique la fonction à chaque ligne du df\n",
    "imdb_dataset[\"review\"][0] # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to clean from non alphanumeric characters\n",
    "    Output: str : The same strings without non alphanumeric characters\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    return re.sub(r\"[^0-9a-zA-Z]+\", \" \", text) # utilisation d'une expression régulière : remplacé par un espace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the other reviewers has mentioned that after watching just 1 Oz episode you ll be hooked They are right as this is exactly what happened with me The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the word It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to many Aryans Muslims gangstas Latinos Christians Italians Irish and more so scuffles death stares dodgy dealings and shady agreements are never far away I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare Forget pretty pictures painted for mainstream audiences forget charm forget romance OZ doesn t mess around The first episode I ever saw struck me as so nasty it was surreal I couldn t say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards who ll be sold out for a nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_special_characters)\n",
    "imdb_dataset[\"review\"][0] # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to lowercase\n",
    "    Output: str : The same string lowercased\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the other reviewers has mentioned that after watching just 1 oz episode you ll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to many aryans muslims gangstas latinos christians italians irish and more so scuffles death stares dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare forget pretty pictures painted for mainstream audiences forget charm forget romance oz doesn t mess around the first episode i ever saw struck me as so nasty it was surreal i couldn t say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards who ll be sold out for a nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(lowercase_text)\n",
    "imdb_dataset[\"review\"][0] # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to tokenize\n",
    "    Output: list of str : A list of the tokens splitted from the input string\n",
    "    \"\"\"\n",
    "    # la phrase devient une liste de mots\n",
    "    return nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[a, wonderful, little, production, the, filmin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basically, there, s, a, family, where, a, lit...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, mattei, s, love, in, the, time, of, m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[probably, my, all, time, favorite, movie, a, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[i, sure, would, like, to, see, a, resurrectio...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[this, show, was, an, amazing, fresh, innovati...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[encouraged, by, the, positive, comments, abou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[if, you, like, original, gut, wrenching, laug...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [one, of, the, other, reviewers, has, mentione...  positive\n",
       "1  [a, wonderful, little, production, the, filmin...  positive\n",
       "2  [i, thought, this, was, a, wonderful, way, to,...  positive\n",
       "3  [basically, there, s, a, family, where, a, lit...  negative\n",
       "4  [petter, mattei, s, love, in, the, time, of, m...  positive\n",
       "5  [probably, my, all, time, favorite, movie, a, ...  positive\n",
       "6  [i, sure, would, like, to, see, a, resurrectio...  positive\n",
       "7  [this, show, was, an, amazing, fresh, innovati...  negative\n",
       "8  [encouraged, by, the, positive, comments, abou...  negative\n",
       "9  [if, you, like, original, gut, wrenching, laug...  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(tokenize_words)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_list):\n",
    "    \"\"\"\n",
    "    Input: list of str : A list of tokens\n",
    "    Output: list of str : The new list with removed stopwords tokens\n",
    "    \"\"\"\n",
    "    # on enlève les mots de liaisons, les pronoms etc.\n",
    "    list_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [w for w in token_list if not w in list_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, reviewers, mentioned, watching, 1, oz, e...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wonderful, little, production, filming, techn...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basically, family, little, boy, jake, thinks,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, mattei, love, time, money, visually, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[probably, time, favorite, movie, story, selfl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70, f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[encouraged, positive, comments, film, looking...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[like, original, gut, wrenching, laughter, lik...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [one, reviewers, mentioned, watching, 1, oz, e...  positive\n",
       "1  [wonderful, little, production, filming, techn...  positive\n",
       "2  [thought, wonderful, way, spend, time, hot, su...  positive\n",
       "3  [basically, family, little, boy, jake, thinks,...  negative\n",
       "4  [petter, mattei, love, time, money, visually, ...  positive\n",
       "5  [probably, time, favorite, movie, story, selfl...  positive\n",
       "6  [sure, would, like, see, resurrection, dated, ...  positive\n",
       "7  [show, amazing, fresh, innovative, idea, 70, f...  negative\n",
       "8  [encouraged, positive, comments, film, looking...  negative\n",
       "9  [like, original, gut, wrenching, laughter, lik...  positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_stopwords)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list des racines \"neutres\" des mots de la phrase : certains mots n'existent pas\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stem_words = [stemmer.stem(w) for w in imdb_dataset[\"review\"][0]] \n",
    "\n",
    "# liste des mots \"neutres\" de la phrase : tous les mots existent, les verbes sont encore conjugués.\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lem_words = [lemmatizer.lemmatize(w) for w in imdb_dataset[\"review\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(token_list):\n",
    "    \"\"\"\n",
    "    Input: list of str : A list of tokens to stem\n",
    "    Output: list of str : The list of stemmed tokens\n",
    "    \"\"\"\n",
    "    # on dé-conjugue les verbes, on enlève les pluriels, on enlève les masculins/féminims\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    stem_words = [stemmer.stem(w) for w in token_list]\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, review, mention, watch, 1, oz, episod, h...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wonder, littl, product, film, techniqu, unass...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basic, famili, littl, boy, jake, think, zombi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, mattei, love, time, money, visual, st...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[probabl, time, favorit, movi, stori, selfless...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[sure, would, like, see, resurrect, date, seah...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, amaz, fresh, innov, idea, 70, first, ai...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[encourag, posit, comment, film, look, forward...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[like, origin, gut, wrench, laughter, like, mo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [one, review, mention, watch, 1, oz, episod, h...  positive\n",
       "1  [wonder, littl, product, film, techniqu, unass...  positive\n",
       "2  [thought, wonder, way, spend, time, hot, summe...  positive\n",
       "3  [basic, famili, littl, boy, jake, think, zombi...  negative\n",
       "4  [petter, mattei, love, time, money, visual, st...  positive\n",
       "5  [probabl, time, favorit, movi, stori, selfless...  positive\n",
       "6  [sure, would, like, see, resurrect, date, seah...  positive\n",
       "7  [show, amaz, fresh, innov, idea, 70, first, ai...  negative\n",
       "8  [encourag, posit, comment, film, look, forward...  negative\n",
       "9  [like, origin, gut, wrench, laughter, like, mo...  positive"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(stem_words)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join all that together and apply it to our dataset. The following function simply chains all the preprocessing steps you just implemented. \n",
    "\n",
    "It adds the `list_output` flag, if False it will reconcatenate all the preprocessed tokens into a single string (with spaces between tokens), if True it will keep each sentence as a list of tokens. Depending on the libraries you will use for the next steps, it can be useful to have one or the other representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_dataset(dataset, text_col_name = 'review', html_tags = True,\n",
    "                           special_chars = True, lowercase = True , stemming = True , \n",
    "                           stopwords = True, list_output = False ):\n",
    "    \"\"\"\n",
    "    Apply the choosen preprocessing steps to a corpus of texts and return the \n",
    "    preprocessed corpus. The list_output flag allows to return either a list\n",
    "    of token, or a rejoined string with spaces between the preprocessed tokens.\n",
    "    \"\"\"\n",
    "    def rejoin_text(token_list):\n",
    "        return ' '.join(token_list)\n",
    "    \n",
    "    \n",
    "    output = dataset.copy()\n",
    "    \n",
    "    if html_tags : \n",
    "        output[text_col_name] = output[text_col_name].apply(remove_html_tags)\n",
    "        \n",
    "    if special_chars :\n",
    "        output[text_col_name] = output[text_col_name].apply(remove_special_characters)\n",
    "        \n",
    "    if lowercase :\n",
    "        output[text_col_name] = output[text_col_name].apply(lowercase_text)\n",
    "    \n",
    "    #Tokenization for next steps:\n",
    "    output[text_col_name] = output[text_col_name].apply(tokenize_words)\n",
    "    \n",
    "    if stopwords :\n",
    "        output[text_col_name] = output[text_col_name].apply(remove_stopwords)\n",
    "        \n",
    "    if stemming :\n",
    "        output[text_col_name] = output[text_col_name].apply(stem_words)\n",
    "        \n",
    "    if not list_output :\n",
    "        output[text_col_name] = output[text_col_name].apply(rejoin_text)\n",
    "        \n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "                           special_chars = True, lowercase = True , stemming = True , \n",
    "                           stopwords = True, list_output = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review mention watch 1 oz episod hook righ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonder littl product film techniqu unassum old...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic famili littl boy jake think zombi closet...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probabl time favorit movi stori selfless sacri...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sure would like see resurrect date seahunt ser...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>show amaz fresh innov idea 70 first air first ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encourag posit comment film look forward watch...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like origin gut wrench laughter like movi youn...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one review mention watch 1 oz episod hook righ...  positive\n",
       "1  wonder littl product film techniqu unassum old...  positive\n",
       "2  thought wonder way spend time hot summer weeke...  positive\n",
       "3  basic famili littl boy jake think zombi closet...  negative\n",
       "4  petter mattei love time money visual stun film...  positive\n",
       "5  probabl time favorit movi stori selfless sacri...  positive\n",
       "6  sure would like see resurrect date seahunt ser...  positive\n",
       "7  show amaz fresh innov idea 70 first air first ...  negative\n",
       "8  encourag posit comment film look forward watch...  negative\n",
       "9  like origin gut wrench laughter like movi youn...  positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_clean_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'imdb_clean_dataset' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# on stocke la variable clean au cas où le notebook crashe - on aura pas besoin de refaire toutes les étapes d'avant\n",
    "%store imdb_clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère la variable stockée\n",
    "%store -r imdb_clean_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text classification with Bag-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we have cleaned the reviews of our dataset, how do we represent them as vectors in order to classify it ? \n",
    "One classic way to achieve that is the Bag-Of-Words (BOW) approach. To encode a text in a bag of word, we first need to know all the different words $w$ that appear in all our reviews, called the vocabulary : $w \\in \\mathcal{V}$. For each word $w$ we attribute an index $idx(w) = i$ with $i \\in \\{0, |\\mathcal{V}|-1\\}$, and represent a review $r$ as a vector of the size of the vocabulary $x_r \\in \\mathbb{R}^{|\\mathcal{V}|}$. To encode a review we are simply going to count how many time each word appears and assign it at its corresponding index in the bag-of-words vector : $x_{r,i} = count(w,r)$, where i = idx(w). \n",
    "\n",
    "This means that we completely disregard the words order, and simply take into account the number of times each word appears in each review to represent them. There are many variations of this concept, TF-IDF (term frequency-inverse document frequency) for example, gives more weight to uncommon words. Read more about BOW and TF-IDF there:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "Let's start with bag-of-words. In general we don't consider the whole vocabulary but only some of the most frequent words in order to reduce the dimensionality and avoid noise from rare words. Here we will only consider the 25000 most frequent words of the training set, meaning the words that are only in the test set will be ignored. Thus we have : $x_r \\in \\mathbb{R^{25000}}$.\n",
    "\n",
    "Encode all the reviews as bag-of-words, and train and evaluate a logistic regression model on the following train test splits. As we have seen previously, if we wanted to investigate this model we should also grid search for hyperparameters by doing a cross-validation with validation sets, etc. However this is not the goal today, so we'll simply go for a train/test split for this experiment. Concerning the evaluation metrics, in this case we care equally about correctly predicting the positives and the negatives, and we have a balanced dataset, thus we can simply use accuracy this time.\n",
    "\n",
    "Once again, don't do everything from scratch and try to find libraries that propose implementations of these concepts !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 25000 \n",
    "\n",
    "#Train/test split:\n",
    "\n",
    "lb=LabelBinarizer()\n",
    "sentiment_labels=lb.fit_transform(imdb_clean_dataset['sentiment']) # y binarisé\n",
    "\n",
    "train_reviews = imdb_clean_dataset.review[:45000]\n",
    "test_reviews = imdb_clean_dataset.review[45000:]\n",
    "\n",
    "train_sentiments = sentiment_labels[:45000] \n",
    "test_sentiments = sentiment_labels[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BOW - X_TRAIN ###\n",
    "\n",
    "# création du dico qui compte l'occurence de chaque mot dans les reviews\n",
    "vectorizer = CountVectorizer(max_features=max_vocab_size) # on lui dit qu'on veut un dico de 25000 mots maximum\n",
    "X_train = vectorizer.fit_transform(train_reviews)\n",
    "\n",
    "# X_train est ici une matrice sparse (stockage optimisé en mémoire - prend moins de place car \"compresse\" les 0)\n",
    "# si on fait X_train.toarray() : on voit la vraie matrice mais prend énoooormément d'espace de stockage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 15740,\n",
       " 'review': 18471,\n",
       " 'mention': 14135,\n",
       " 'watch': 24116,\n",
       " 'oz': 16077,\n",
       " 'episod': 7346,\n",
       " 'hook': 10539,\n",
       " 'right': 18576,\n",
       " 'exactli': 7546,\n",
       " 'happen': 9882,\n",
       " 'first': 8124,\n",
       " 'thing': 22198,\n",
       " 'struck': 21313,\n",
       " 'brutal': 3247,\n",
       " 'unflinch': 23221,\n",
       " 'scene': 19309,\n",
       " 'violenc': 23841,\n",
       " 'set': 19696,\n",
       " 'word': 24596,\n",
       " 'go': 9185,\n",
       " 'trust': 22800,\n",
       " 'show': 19977,\n",
       " 'faint': 7741,\n",
       " 'heart': 10075,\n",
       " 'timid': 22335,\n",
       " 'pull': 17566,\n",
       " 'punch': 17582,\n",
       " 'regard': 18169,\n",
       " 'drug': 6724,\n",
       " 'sex': 19721,\n",
       " 'hardcor': 9899,\n",
       " 'classic': 4361,\n",
       " 'use': 23523,\n",
       " 'call': 3526,\n",
       " 'nicknam': 15257,\n",
       " 'given': 9111,\n",
       " 'oswald': 15878,\n",
       " 'maximum': 13878,\n",
       " 'secur': 19539,\n",
       " 'state': 21031,\n",
       " 'focus': 8315,\n",
       " 'mainli': 13480,\n",
       " 'emerald': 7167,\n",
       " 'citi': 4312,\n",
       " 'experiment': 7627,\n",
       " 'section': 19536,\n",
       " 'prison': 17351,\n",
       " 'cell': 3885,\n",
       " 'glass': 9129,\n",
       " 'front': 8634,\n",
       " 'face': 7713,\n",
       " 'inward': 11395,\n",
       " 'privaci': 17356,\n",
       " 'high': 10303,\n",
       " 'agenda': 680,\n",
       " 'em': 7140,\n",
       " 'home': 10480,\n",
       " 'mani': 13602,\n",
       " 'aryan': 1473,\n",
       " 'muslim': 14914,\n",
       " 'gangsta': 8808,\n",
       " 'latino': 12595,\n",
       " 'christian': 4225,\n",
       " 'italian': 11475,\n",
       " 'irish': 11418,\n",
       " 'scuffl': 19472,\n",
       " 'death': 5708,\n",
       " 'stare': 21004,\n",
       " 'dodgi': 6443,\n",
       " 'deal': 5699,\n",
       " 'shadi': 19748,\n",
       " 'agreement': 704,\n",
       " 'never': 15202,\n",
       " 'far': 7806,\n",
       " 'away': 1737,\n",
       " 'would': 24632,\n",
       " 'say': 19256,\n",
       " 'main': 13477,\n",
       " 'appeal': 1282,\n",
       " 'due': 6766,\n",
       " 'fact': 7720,\n",
       " 'goe': 9209,\n",
       " 'dare': 5595,\n",
       " 'forget': 8404,\n",
       " 'pretti': 17299,\n",
       " 'pictur': 16747,\n",
       " 'paint': 16122,\n",
       " 'mainstream': 13482,\n",
       " 'audienc': 1630,\n",
       " 'charm': 4020,\n",
       " 'romanc': 18751,\n",
       " 'mess': 14180,\n",
       " 'around': 1435,\n",
       " 'ever': 7504,\n",
       " 'saw': 19248,\n",
       " 'nasti': 15037,\n",
       " 'surreal': 21590,\n",
       " 'readi': 17997,\n",
       " 'develop': 6043,\n",
       " 'tast': 21910,\n",
       " 'got': 9317,\n",
       " 'accustom': 492,\n",
       " 'level': 12824,\n",
       " 'graphic': 9411,\n",
       " 'injustic': 11210,\n",
       " 'crook': 5302,\n",
       " 'guard': 9591,\n",
       " 'sold': 20557,\n",
       " 'nickel': 15254,\n",
       " 'inmat': 11216,\n",
       " 'kill': 12169,\n",
       " 'order': 15816,\n",
       " 'get': 9002,\n",
       " 'well': 24237,\n",
       " 'manner': 13622,\n",
       " 'middl': 14255,\n",
       " 'class': 4359,\n",
       " 'turn': 22861,\n",
       " 'bitch': 2546,\n",
       " 'lack': 12451,\n",
       " 'street': 21262,\n",
       " 'skill': 20233,\n",
       " 'experi': 7625,\n",
       " 'may': 13882,\n",
       " 'becom': 2195,\n",
       " 'comfort': 4655,\n",
       " 'uncomfort': 23062,\n",
       " 'view': 23790,\n",
       " 'that': 22140,\n",
       " 'touch': 22511,\n",
       " 'darker': 5606,\n",
       " 'side': 20056,\n",
       " 'wonder': 24562,\n",
       " 'littl': 13007,\n",
       " 'product': 17387,\n",
       " 'film': 8055,\n",
       " 'techniqu': 21972,\n",
       " 'unassum': 23023,\n",
       " 'old': 15692,\n",
       " 'time': 22328,\n",
       " 'bbc': 2128,\n",
       " 'fashion': 7839,\n",
       " 'give': 9109,\n",
       " 'sometim': 20599,\n",
       " 'discomfort': 6253,\n",
       " 'sens': 19624,\n",
       " 'realism': 18004,\n",
       " 'entir': 7309,\n",
       " 'piec': 16753,\n",
       " 'actor': 526,\n",
       " 'extrem': 7683,\n",
       " 'chosen': 4213,\n",
       " 'michael': 14232,\n",
       " 'sheen': 19847,\n",
       " 'voic': 23922,\n",
       " 'pat': 16331,\n",
       " 'truli': 22792,\n",
       " 'see': 19548,\n",
       " 'seamless': 19501,\n",
       " 'edit': 6970,\n",
       " 'guid': 9608,\n",
       " 'refer': 18145,\n",
       " 'william': 24426,\n",
       " 'diari': 6102,\n",
       " 'entri': 7321,\n",
       " 'worth': 24627,\n",
       " 'terrificli': 22088,\n",
       " 'written': 24673,\n",
       " 'perform': 16546,\n",
       " 'master': 13800,\n",
       " 'great': 9441,\n",
       " 'comedi': 4647,\n",
       " 'life': 12878,\n",
       " 'realli': 18008,\n",
       " 'come': 4644,\n",
       " 'fantasi': 7801,\n",
       " 'rather': 17946,\n",
       " 'tradit': 22561,\n",
       " 'dream': 6658,\n",
       " 'remain': 18260,\n",
       " 'solid': 20567,\n",
       " 'disappear': 6227,\n",
       " 'play': 16892,\n",
       " 'knowledg': 12297,\n",
       " 'particularli': 16294,\n",
       " 'concern': 4757,\n",
       " 'orton': 15854,\n",
       " 'halliwel': 9790,\n",
       " 'flat': 8189,\n",
       " 'mural': 14885,\n",
       " 'decor': 5759,\n",
       " 'everi': 7509,\n",
       " 'surfac': 21573,\n",
       " 'terribl': 22084,\n",
       " 'done': 6508,\n",
       " 'thought': 22232,\n",
       " 'way': 24153,\n",
       " 'spend': 20780,\n",
       " 'hot': 10607,\n",
       " 'summer': 21479,\n",
       " 'weekend': 24201,\n",
       " 'sit': 20185,\n",
       " 'air': 740,\n",
       " 'condit': 4781,\n",
       " 'theater': 22145,\n",
       " 'light': 12893,\n",
       " 'plot': 16928,\n",
       " 'simplist': 20139,\n",
       " 'dialogu': 6094,\n",
       " 'witti': 24526,\n",
       " 'charact': 3992,\n",
       " 'likabl': 12904,\n",
       " 'even': 7500,\n",
       " 'bread': 3063,\n",
       " 'suspect': 21610,\n",
       " 'serial': 19672,\n",
       " 'killer': 12170,\n",
       " 'disappoint': 6228,\n",
       " 'realiz': 18007,\n",
       " 'match': 13817,\n",
       " 'point': 16980,\n",
       " 'risk': 18620,\n",
       " 'addict': 545,\n",
       " 'proof': 17432,\n",
       " 'woodi': 24580,\n",
       " 'allen': 863,\n",
       " 'still': 21146,\n",
       " 'fulli': 8680,\n",
       " 'control': 4945,\n",
       " 'style': 21363,\n",
       " 'us': 23517,\n",
       " 'grown': 9562,\n",
       " 'love': 13196,\n",
       " 'laugh': 12605,\n",
       " 'year': 24773,\n",
       " 'decad': 5731,\n",
       " 'impress': 10993,\n",
       " 'scarlet': 19295,\n",
       " 'johanson': 11773,\n",
       " 'manag': 13566,\n",
       " 'tone': 22440,\n",
       " 'sexi': 19722,\n",
       " 'imag': 10914,\n",
       " 'jump': 11894,\n",
       " 'averag': 1709,\n",
       " 'spirit': 20813,\n",
       " 'young': 24827,\n",
       " 'woman': 24557,\n",
       " 'crown': 5326,\n",
       " 'jewel': 11700,\n",
       " 'career': 3668,\n",
       " 'wittier': 24528,\n",
       " 'devil': 6051,\n",
       " 'wear': 24175,\n",
       " 'prada': 17182,\n",
       " 'interest': 11311,\n",
       " 'superman': 21536,\n",
       " 'friend': 8598,\n",
       " 'basic': 2057,\n",
       " 'famili': 7776,\n",
       " 'boy': 2980,\n",
       " 'jake': 11552,\n",
       " 'think': 22200,\n",
       " 'zombi': 24968,\n",
       " 'closet': 4455,\n",
       " 'parent': 16250,\n",
       " 'fight': 8043,\n",
       " 'movi': 14795,\n",
       " 'slower': 20359,\n",
       " 'soap': 20501,\n",
       " 'opera': 15772,\n",
       " 'suddenli': 21439,\n",
       " 'decid': 5744,\n",
       " 'rambo': 17864,\n",
       " 'ok': 15678,\n",
       " 'make': 13495,\n",
       " 'must': 14917,\n",
       " 'thriller': 22249,\n",
       " 'drama': 6640,\n",
       " 'watchabl': 24117,\n",
       " 'divorc': 6400,\n",
       " 'argu': 1383,\n",
       " 'like': 12905,\n",
       " 'real': 18002,\n",
       " 'total': 22502,\n",
       " 'ruin': 18924,\n",
       " 'expect': 7618,\n",
       " 'boogeyman': 2857,\n",
       " 'similar': 20119,\n",
       " 'instead': 11269,\n",
       " 'meaningless': 14018,\n",
       " 'spot': 20857,\n",
       " '10': 15,\n",
       " 'descent': 5979,\n",
       " 'dialog': 6093,\n",
       " 'shot': 19970,\n",
       " 'ignor': 10881,\n",
       " 'mattei': 13849,\n",
       " 'money': 14606,\n",
       " 'visual': 23882,\n",
       " 'stun': 21343,\n",
       " 'mr': 14812,\n",
       " 'offer': 15645,\n",
       " 'vivid': 23901,\n",
       " 'portrait': 17119,\n",
       " 'human': 10703,\n",
       " 'relat': 18231,\n",
       " 'seem': 19555,\n",
       " 'tell': 22014,\n",
       " 'power': 17170,\n",
       " 'success': 21425,\n",
       " 'peopl': 16512,\n",
       " 'differ': 6138,\n",
       " 'situat': 20192,\n",
       " 'encount': 7222,\n",
       " 'variat': 23629,\n",
       " 'arthur': 1457,\n",
       " 'theme': 22154,\n",
       " 'director': 6212,\n",
       " 'transfer': 22597,\n",
       " 'action': 523,\n",
       " 'present': 17274,\n",
       " 'new': 15209,\n",
       " 'york': 24822,\n",
       " 'meet': 14061,\n",
       " 'connect': 4837,\n",
       " 'anoth': 1173,\n",
       " 'next': 15234,\n",
       " 'person': 16586,\n",
       " 'know': 12294,\n",
       " 'previou': 17308,\n",
       " 'contact': 4907,\n",
       " 'stylishli': 21366,\n",
       " 'sophist': 20631,\n",
       " 'luxuri': 13331,\n",
       " 'look': 13123,\n",
       " 'taken': 21816,\n",
       " 'live': 13012,\n",
       " 'world': 24611,\n",
       " 'habitat': 9719,\n",
       " 'soul': 20663,\n",
       " 'stage': 20941,\n",
       " 'loneli': 13108,\n",
       " 'inhabit': 11196,\n",
       " 'big': 2471,\n",
       " 'best': 2404,\n",
       " 'place': 16855,\n",
       " 'find': 8078,\n",
       " 'sincer': 20148,\n",
       " 'fulfil': 8676,\n",
       " 'discern': 6244,\n",
       " 'case': 3753,\n",
       " 'act': 522,\n",
       " 'good': 9257,\n",
       " 'direct': 6209,\n",
       " 'steve': 21122,\n",
       " 'buscemi': 3407,\n",
       " 'rosario': 18804,\n",
       " 'dawson': 5669,\n",
       " 'carol': 3711,\n",
       " 'kane': 11966,\n",
       " 'imperioli': 10964,\n",
       " 'adrian': 592,\n",
       " 'grenier': 9478,\n",
       " 'rest': 18405,\n",
       " 'talent': 21824,\n",
       " 'cast': 3775,\n",
       " 'aliv': 852,\n",
       " 'wish': 24500,\n",
       " 'luck': 13260,\n",
       " 'await': 1731,\n",
       " 'anxious': 1231,\n",
       " 'work': 24602,\n",
       " 'probabl': 17367,\n",
       " 'favorit': 7875,\n",
       " 'stori': 21207,\n",
       " 'selfless': 19589,\n",
       " 'sacrific': 19019,\n",
       " 'dedic': 5767,\n",
       " 'nobl': 15346,\n",
       " 'caus': 3835,\n",
       " 'preachi': 17204,\n",
       " 'bore': 2897,\n",
       " 'despit': 6002,\n",
       " 'seen': 19559,\n",
       " '15': 56,\n",
       " 'last': 12579,\n",
       " '25': 235,\n",
       " 'paul': 16373,\n",
       " 'luka': 13279,\n",
       " 'bring': 3155,\n",
       " 'tear': 21960,\n",
       " 'eye': 7689,\n",
       " 'bett': 2421,\n",
       " 'davi': 5657,\n",
       " 'sympathet': 21737,\n",
       " 'role': 18740,\n",
       " 'delight': 5846,\n",
       " 'kid': 12149,\n",
       " 'grandma': 9396,\n",
       " 'dress': 6679,\n",
       " 'midget': 14259,\n",
       " 'children': 4145,\n",
       " 'fun': 8684,\n",
       " 'mother': 14760,\n",
       " 'slow': 20358,\n",
       " 'awaken': 1733,\n",
       " 'roof': 18778,\n",
       " 'believ': 2281,\n",
       " 'startl': 21022,\n",
       " 'dozen': 6619,\n",
       " 'thumb': 22273,\n",
       " 'sure': 21570,\n",
       " 'resurrect': 18420,\n",
       " 'date': 5647,\n",
       " 'seri': 19671,\n",
       " 'tech': 21966,\n",
       " 'today': 22395,\n",
       " 'back': 1805,\n",
       " 'excit': 7566,\n",
       " 'grew': 9482,\n",
       " 'black': 2572,\n",
       " 'white': 24331,\n",
       " 'tv': 22882,\n",
       " 'gunsmok': 9665,\n",
       " 'hero': 10241,\n",
       " 'week': 24199,\n",
       " 'vote': 23953,\n",
       " 'comeback': 4645,\n",
       " 'sea': 19489,\n",
       " 'hunt': 10744,\n",
       " 'need': 15121,\n",
       " 'chang': 3972,\n",
       " 'pace': 16085,\n",
       " 'water': 24123,\n",
       " 'adventur': 611,\n",
       " 'oh': 15662,\n",
       " 'thank': 22135,\n",
       " 'outlet': 15928,\n",
       " 'viewpoint': 23793,\n",
       " 'ole': 15699,\n",
       " 'wanna': 24056,\n",
       " 'nice': 15246,\n",
       " 'read': 17994,\n",
       " 'plu': 16938,\n",
       " 'rhyme': 18516,\n",
       " 'line': 12952,\n",
       " 'let': 12808,\n",
       " 'submit': 21390,\n",
       " 'leav': 12687,\n",
       " 'doubt': 6578,\n",
       " 'quit': 17742,\n",
       " 'amaz': 959,\n",
       " 'fresh': 8574,\n",
       " 'innov': 11225,\n",
       " 'idea': 10853,\n",
       " '70': 332,\n",
       " 'brilliant': 3150,\n",
       " 'drop': 6714,\n",
       " '1990': 191,\n",
       " 'funni': 8696,\n",
       " 'anymor': 1236,\n",
       " 'continu': 4926,\n",
       " 'declin': 5753,\n",
       " 'complet': 4713,\n",
       " 'wast': 24110,\n",
       " 'disgrac': 6280,\n",
       " 'fallen': 7763,\n",
       " 'write': 24670,\n",
       " 'pain': 16117,\n",
       " 'bad': 1833,\n",
       " 'almost': 894,\n",
       " 'mildli': 14307,\n",
       " 'entertain': 7302,\n",
       " 'respit': 18400,\n",
       " 'guest': 9601,\n",
       " 'host': 10602,\n",
       " 'hard': 9896,\n",
       " 'creator': 5229,\n",
       " 'hand': 9834,\n",
       " 'select': 19583,\n",
       " 'origin': 15834,\n",
       " 'also': 913,\n",
       " 'chose': 4212,\n",
       " 'band': 1938,\n",
       " 'hack': 9721,\n",
       " 'follow': 8334,\n",
       " 'recogn': 18071,\n",
       " 'brillianc': 3149,\n",
       " 'fit': 8141,\n",
       " 'replac': 18338,\n",
       " 'mediocr': 14047,\n",
       " 'felt': 7933,\n",
       " 'star': 20997,\n",
       " 'respect': 18398,\n",
       " 'made': 13413,\n",
       " 'huge': 10684,\n",
       " 'aw': 1730,\n",
       " 'encourag': 7223,\n",
       " 'posit': 17129,\n",
       " 'comment': 4664,\n",
       " 'forward': 8442,\n",
       " 'mistak': 14485,\n",
       " 'worst': 24626,\n",
       " 'storylin': 21215,\n",
       " 'soundtrack': 20669,\n",
       " 'song': 20608,\n",
       " 'lame': 12495,\n",
       " 'countri': 5121,\n",
       " 'tune': 22840,\n",
       " 'less': 12800,\n",
       " 'four': 8457,\n",
       " 'cheap': 4054,\n",
       " 'rare': 17925,\n",
       " 'happi': 9884,\n",
       " 'end': 7228,\n",
       " 'credit': 5235,\n",
       " 'prevent': 17306,\n",
       " 'score': 19403,\n",
       " 'harvey': 9963,\n",
       " 'keitel': 12074,\n",
       " 'least': 12681,\n",
       " 'bit': 2545,\n",
       " 'effort': 6997,\n",
       " 'obsess': 15585,\n",
       " 'gut': 9680,\n",
       " 'wrench': 24652,\n",
       " 'laughter': 12611,\n",
       " 'hell': 10160,\n",
       " 'mom': 14585,\n",
       " 'camp': 3566,\n",
       " 'phil': 16673,\n",
       " 'alien': 843,\n",
       " 'quirki': 17740,\n",
       " 'humour': 10723,\n",
       " 'base': 2047,\n",
       " 'odd': 15617,\n",
       " 'everyth': 7515,\n",
       " 'actual': 528,\n",
       " 'punchlin': 17584,\n",
       " 'progress': 17404,\n",
       " 'joke': 11788,\n",
       " 'low': 13216,\n",
       " 'budget': 3285,\n",
       " 'problem': 17371,\n",
       " 'eventu': 7503,\n",
       " 'lost': 13163,\n",
       " 'imagin': 10916,\n",
       " 'stoner': 21192,\n",
       " 'current': 5428,\n",
       " 'partak': 16286,\n",
       " 'someth': 20598,\n",
       " 'better': 2423,\n",
       " 'tri': 22687,\n",
       " 'brother': 3222,\n",
       " 'planet': 16868,\n",
       " '12': 38,\n",
       " 'came': 3553,\n",
       " 'recal': 18046,\n",
       " 'scariest': 19293,\n",
       " 'bird': 2526,\n",
       " 'eat': 6917,\n",
       " 'men': 14117,\n",
       " 'dangl': 5569,\n",
       " 'helplessli': 10180,\n",
       " 'parachut': 16216,\n",
       " 'horror': 10583,\n",
       " 'cheesi': 4081,\n",
       " 'saturday': 19222,\n",
       " 'afternoon': 665,\n",
       " 'tire': 22358,\n",
       " 'formula': 8423,\n",
       " 'monster': 14641,\n",
       " 'type': 22929,\n",
       " 'usual': 23532,\n",
       " 'includ': 11050,\n",
       " 'beauti': 2175,\n",
       " 'might': 14278,\n",
       " 'daughter': 5650,\n",
       " 'professor': 17392,\n",
       " 'resolut': 18392,\n",
       " 'die': 6128,\n",
       " 'care': 3666,\n",
       " 'much': 14825,\n",
       " 'romant': 18756,\n",
       " 'angl': 1115,\n",
       " 'predict': 17227,\n",
       " 'unintent': 23271,\n",
       " 'humor': 10720,\n",
       " 'later': 12587,\n",
       " 'psycho': 17523,\n",
       " 'janet': 11571,\n",
       " 'leigh': 12733,\n",
       " 'bump': 3348,\n",
       " 'earli': 6890,\n",
       " 'sat': 19205,\n",
       " 'took': 22449,\n",
       " 'notic': 15438,\n",
       " 'sinc': 20147,\n",
       " 'screenwrit': 19444,\n",
       " 'scari': 19291,\n",
       " 'possibl': 17132,\n",
       " 'worn': 24618,\n",
       " 'rule': 18927,\n",
       " 'im': 10913,\n",
       " 'fan': 7782,\n",
       " 'boll': 2808,\n",
       " 'enjoy': 7270,\n",
       " 'postal': 17134,\n",
       " 'mayb': 13886,\n",
       " 'appar': 1278,\n",
       " 'bought': 2942,\n",
       " 'cri': 5262,\n",
       " 'long': 13112,\n",
       " 'ago': 696,\n",
       " 'game': 8792,\n",
       " 'infiltr': 11162,\n",
       " 'secret': 19532,\n",
       " 'research': 18376,\n",
       " 'lab': 12434,\n",
       " 'locat': 13058,\n",
       " 'tropic': 22764,\n",
       " 'island': 11463,\n",
       " 'warn': 24087,\n",
       " 'scheme': 19324,\n",
       " 'togeth': 22404,\n",
       " 'along': 898,\n",
       " 'legion': 12720,\n",
       " 'schmuck': 19348,\n",
       " 'feel': 7912,\n",
       " 'invit': 11390,\n",
       " 'three': 22242,\n",
       " 'countrymen': 5122,\n",
       " 'player': 16895,\n",
       " 'name': 14987,\n",
       " 'til': 22317,\n",
       " 'schweiger': 19378,\n",
       " 'udo': 22958,\n",
       " 'kier': 12161,\n",
       " 'ralf': 17853,\n",
       " 'self': 19586,\n",
       " 'biz': 2558,\n",
       " 'tale': 21823,\n",
       " 'jack': 11512,\n",
       " 'carver': 3747,\n",
       " 'ye': 24769,\n",
       " 'german': 8986,\n",
       " 'hail': 9753,\n",
       " 'dude': 6763,\n",
       " 'howev': 10642,\n",
       " 'badass': 1835,\n",
       " 'complain': 4709,\n",
       " 'stay': 21051,\n",
       " 'true': 22786,\n",
       " 'whole': 24352,\n",
       " 'perspect': 16592,\n",
       " 'kick': 12146,\n",
       " 'beyond': 2438,\n",
       " 'dement': 5875,\n",
       " 'evil': 7524,\n",
       " 'mad': 13403,\n",
       " 'scientist': 19386,\n",
       " 'dr': 6623,\n",
       " 'genet': 8936,\n",
       " 'mutat': 14925,\n",
       " 'soldier': 20559,\n",
       " 'gm': 9180,\n",
       " 'top': 22462,\n",
       " 'remind': 18276,\n",
       " 'spoiler': 20836,\n",
       " 'vancouv': 23608,\n",
       " 'reason': 18020,\n",
       " 'palm': 16152,\n",
       " 'tree': 22660,\n",
       " 'rich': 18531,\n",
       " 'wood': 24573,\n",
       " 'gone': 9248,\n",
       " 'start': 21020,\n",
       " 'cannot': 3604,\n",
       " 'shenanigan': 19871,\n",
       " 'deliv': 5853,\n",
       " 'mean': 14011,\n",
       " 'suck': 21433,\n",
       " 'impli': 10974,\n",
       " 'area': 1372,\n",
       " 'boat': 2748,\n",
       " 'albino': 798,\n",
       " 'squad': 20894,\n",
       " 'enter': 7300,\n",
       " 'reek': 18136,\n",
       " 'poop': 17068,\n",
       " 'simpleton': 20134,\n",
       " 'fa': 7704,\n",
       " 'take': 21814,\n",
       " 'ahead': 712,\n",
       " 'btw': 3259,\n",
       " 'annoy': 1164,\n",
       " 'sidekick': 20059,\n",
       " 'shoot': 19947,\n",
       " 'minut': 14407,\n",
       " 'screen': 19440,\n",
       " 'shakespear': 19764,\n",
       " 'appreci': 1303,\n",
       " 'mass': 13789,\n",
       " 'scottish': 19415,\n",
       " 'certain': 3917,\n",
       " 'rev': 18455,\n",
       " 'bowdler': 2967,\n",
       " 'henc': 10189,\n",
       " 'victorian': 23770,\n",
       " 'era': 7362,\n",
       " 'improv': 11001,\n",
       " 'perfect': 16543,\n",
       " 'ten': 22037,\n",
       " 'text': 22118,\n",
       " 'english': 7257,\n",
       " 'composit': 4728,\n",
       " 'fort': 8430,\n",
       " 'keep': 12066,\n",
       " 'cut': 5451,\n",
       " 'fantast': 7804,\n",
       " 'famou': 7780,\n",
       " 'georg': 8972,\n",
       " 'clooney': 4450,\n",
       " 'roll': 18742,\n",
       " 'man': 13565,\n",
       " 'constant': 4886,\n",
       " 'sorrow': 20646,\n",
       " 'everybodi': 7511,\n",
       " 'greet': 9467,\n",
       " 'bart': 2036,\n",
       " 'kind': 12192,\n",
       " 'drawn': 6655,\n",
       " 'erot': 7388,\n",
       " 'amateurish': 955,\n",
       " 'unbeliev': 23038,\n",
       " 'sort': 20647,\n",
       " 'school': 19358,\n",
       " 'project': 17406,\n",
       " 'rosanna': 18802,\n",
       " 'arquett': 1437,\n",
       " 'stock': 21168,\n",
       " 'bizarr': 2559,\n",
       " 'suppos': 21558,\n",
       " 'midwest': 14272,\n",
       " 'town': 22536,\n",
       " 'involv': 11393,\n",
       " 'lesson': 12803,\n",
       " 'learn': 12676,\n",
       " 'insight': 11247,\n",
       " 'stilt': 21149,\n",
       " 'ridicul': 18558,\n",
       " 'lot': 13164,\n",
       " 'skin': 20237,\n",
       " 'intrigu': 11358,\n",
       " 'videotap': 23778,\n",
       " 'nonsens': 15378,\n",
       " 'bisexu': 2538,\n",
       " 'relationship': 18232,\n",
       " 'nowher': 15467,\n",
       " 'heterosexu': 10262,\n",
       " 'absurd': 454,\n",
       " 'danc': 5557,\n",
       " 'stereotyp': 21112,\n",
       " 'pass': 16308,\n",
       " 'million': 14337,\n",
       " 'mile': 14309,\n",
       " 'could': 5098,\n",
       " 'spent': 20782,\n",
       " 'starv': 21024,\n",
       " 'aid': 722,\n",
       " 'africa': 655,\n",
       " 'simpli': 20135,\n",
       " 'remad': 18259,\n",
       " 'fail': 7739,\n",
       " 'captur': 3649,\n",
       " 'flavor': 8200,\n",
       " 'terror': 22091,\n",
       " '1963': 164,\n",
       " 'titl': 22367,\n",
       " 'liam': 12847,\n",
       " 'neeson': 15128,\n",
       " 'excel': 7559,\n",
       " 'alway': 941,\n",
       " 'hold': 10445,\n",
       " 'except': 7561,\n",
       " 'owen': 16061,\n",
       " 'wilson': 24437,\n",
       " 'luke': 13280,\n",
       " 'major': 13493,\n",
       " 'fault': 7863,\n",
       " 'version': 23725,\n",
       " 'stray': 21256,\n",
       " 'shirley': 19918,\n",
       " 'jackson': 11522,\n",
       " 'attempt': 1602,\n",
       " 'grandios': 9394,\n",
       " 'thrill': 22248,\n",
       " 'earlier': 6891,\n",
       " 'trade': 22558,\n",
       " 'special': 20747,\n",
       " 'effect': 6991,\n",
       " 'friction': 8589,\n",
       " 'older': 15694,\n",
       " 'horribl': 10576,\n",
       " 'chanc': 3963,\n",
       " 'busi': 3415,\n",
       " 'run': 18937,\n",
       " 'sword': 21706,\n",
       " 'emot': 7186,\n",
       " 'attach': 1598,\n",
       " 'machin': 13383,\n",
       " 'want': 24060,\n",
       " 'destroy': 6010,\n",
       " 'blatantli': 2624,\n",
       " 'stolen': 21183,\n",
       " 'lotr': 13166,\n",
       " 'war': 24063,\n",
       " 'matrix': 13844,\n",
       " 'exampl': 7552,\n",
       " 'ghost': 9015,\n",
       " 'final': 8073,\n",
       " 'yoda': 24810,\n",
       " 'vader': 23558,\n",
       " 'spider': 20791,\n",
       " 'begin': 2239,\n",
       " 'frodo': 8626,\n",
       " 'attack': 1599,\n",
       " 'return': 18450,\n",
       " 'king': 12202,\n",
       " 'elijah': 7092,\n",
       " 'victim': 23766,\n",
       " 'wait': 24008,\n",
       " 'hypnot': 10808,\n",
       " 'sting': 21152,\n",
       " 'wrap': 24644,\n",
       " 'uh': 22968,\n",
       " 'hello': 10170,\n",
       " 'vs': 23964,\n",
       " 'termin': 22074,\n",
       " 'someon': 20593,\n",
       " 'nazi': 15083,\n",
       " 'juvenil': 11924,\n",
       " 'rush': 18951,\n",
       " 'conclus': 4768,\n",
       " 'adult': 600,\n",
       " 'either': 7036,\n",
       " 'save': 19240,\n",
       " 'rememb': 18271,\n",
       " 'cinema': 4284,\n",
       " 'dark': 5604,\n",
       " 'nervou': 15174,\n",
       " '74': 338,\n",
       " '75': 340,\n",
       " 'dad': 5504,\n",
       " 'sister': 20182,\n",
       " 'england': 7255,\n",
       " 'tiger': 22309,\n",
       " 'snow': 20481,\n",
       " 'appear': 1284,\n",
       " 'grizzli': 9528,\n",
       " 'adam': 535,\n",
       " 'dan': 5555,\n",
       " 'anyon': 1237,\n",
       " 'dvd': 6849,\n",
       " 'etc': 7447,\n",
       " 'pleas': 16907,\n",
       " 'club': 4472,\n",
       " 'shame': 19779,\n",
       " 'nearest': 15099,\n",
       " '20': 205,\n",
       " 'hear': 10069,\n",
       " 'other': 15882,\n",
       " 'stinker': 21157,\n",
       " 'nomin': 15366,\n",
       " 'golden': 9226,\n",
       " 'globe': 9159,\n",
       " 'femal': 7936,\n",
       " 'renaiss': 18289,\n",
       " 'painter': 16124,\n",
       " 'mangl': 13592,\n",
       " 'recognit': 18073,\n",
       " 'complaint': 4710,\n",
       " 'liberti': 12858,\n",
       " 'perfectli': 16545,\n",
       " 'fine': 8080,\n",
       " 'account': 487,\n",
       " 'artist': 1466,\n",
       " 'dishwat': 6293,\n",
       " 'dull': 6784,\n",
       " 'script': 19455,\n",
       " 'enough': 7280,\n",
       " 'nake': 14984,\n",
       " 'factual': 7727,\n",
       " 'hurriedli': 10759,\n",
       " 'cap': 3619,\n",
       " 'summari': 21475,\n",
       " 'coupl': 5125,\n",
       " 'hour': 10624,\n",
       " 'favor': 7874,\n",
       " 'breviti': 3108,\n",
       " 'sequel': 19652,\n",
       " 'surpris': 21588,\n",
       " 'glut': 9178,\n",
       " 'cash': 3755,\n",
       " 'wrong': 24674,\n",
       " 'guy': 9689,\n",
       " 'concept': 4755,\n",
       " 'cliffhang': 4419,\n",
       " 'mountain': 14781,\n",
       " 'rescu': 18374,\n",
       " 'sli': 20313,\n",
       " 'stop': 21201,\n",
       " 'stallon': 20967,\n",
       " 'nit': 15328,\n",
       " 'picker': 16736,\n",
       " 'especi': 7417,\n",
       " 'expert': 7628,\n",
       " 'climb': 4426,\n",
       " 'aviat': 1715,\n",
       " 'facial': 7717,\n",
       " 'express': 7652,\n",
       " 'full': 8677,\n",
       " 'excus': 7580,\n",
       " 'dismiss': 6309,\n",
       " 'overblown': 15970,\n",
       " 'pile': 16771,\n",
       " 'junk': 11905,\n",
       " 'hors': 10585,\n",
       " 'lovabl': 13195,\n",
       " 'undeni': 23095,\n",
       " 'romp': 18766,\n",
       " 'plenti': 16917,\n",
       " 'john': 11777,\n",
       " 'lithgow': 13003,\n",
       " 'tick': 22294,\n",
       " 'box': 2977,\n",
       " 'baddi': 1839,\n",
       " 'perman': 16564,\n",
       " 'harass': 9892,\n",
       " 'hapless': 9881,\n",
       " 'turncoat': 22862,\n",
       " 'agent': 681,\n",
       " 'rex': 18495,\n",
       " 'linn': 12967,\n",
       " 'traver': 22636,\n",
       " 'henri': 10203,\n",
       " 'rooker': 18781,\n",
       " 'noteworthi': 15435,\n",
       " 'cring': 5272,\n",
       " 'worthi': 24628,\n",
       " 'hal': 9771,\n",
       " 'insist': 11252,\n",
       " 'constantli': 4888,\n",
       " 'shriek': 20000,\n",
       " 'disbelief': 6240,\n",
       " 'captor': 3648,\n",
       " 'hurt': 10761,\n",
       " 'anybodi': 1233,\n",
       " 'whilst': 24303,\n",
       " 'ralph': 17855,\n",
       " 'frank': 8507,\n",
       " 'grin': 9507,\n",
       " 'girl': 9094,\n",
       " 'plummet': 16947,\n",
       " 'former': 8418,\n",
       " 'london': 13106,\n",
       " 'burn': 3391,\n",
       " 'craig': 5185,\n",
       " 'brit': 3166,\n",
       " 'cropper': 5306,\n",
       " 'footbal': 8355,\n",
       " 'help': 10177,\n",
       " 'judgement': 11866,\n",
       " 'lower': 13220,\n",
       " 'volum': 23940,\n",
       " 'helicopt': 10158,\n",
       " 'misfortun': 14449,\n",
       " 'entireti': 7310,\n",
       " 'aspect': 1515,\n",
       " 'paper': 16203,\n",
       " 'thin': 22196,\n",
       " 'sequenc': 19653,\n",
       " 'fake': 7755,\n",
       " 'pack': 16091,\n",
       " 'crappi': 5199,\n",
       " 'liner': 12956,\n",
       " 'amus': 1039,\n",
       " 'gear': 8900,\n",
       " 'toward': 22532,\n",
       " 'women': 24561,\n",
       " 'utterli': 23542,\n",
       " 'unattract': 23027,\n",
       " 'wrinkl': 24666,\n",
       " 'weird': 24221,\n",
       " 'costum': 5087,\n",
       " 'miser': 14445,\n",
       " 'absolut': 447,\n",
       " 'hr': 10652,\n",
       " 'regret': 18187,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico_mots = vectorizer.vocabulary_ # attribut permettant d'accéder au dico des occurences de chaque mot\n",
    "dico_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BOW - X_TEST ###\n",
    "\n",
    "X_test = vectorizer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 25000), (5000, 25000))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_sentiments.ravel() # ravel() permet de passer de (row, 1) à (row,) en shape\n",
    "y_test = test_sentiments.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000,), (5000,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d5learner-15/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BOW - LOGISTIC REGRESSION ###\n",
    "\n",
    "model_0 = LogisticRegression() # max_iter = 100. On peut s'amuser à augmenter le nb d'itérations\n",
    "model_0.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8818"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prédiction et évaluation\n",
    "\n",
    "y_pred_0 = model_0.predict(X_test)\n",
    "\n",
    "score_0 = accuracy_score(y_test, y_pred_0)\n",
    "score_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get about 88% accuracy, pretty good for such a simple model. Now let's do the same with a tf-idf encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF IDF - X_TRAIN & X_TEST ###\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = max_vocab_size)\n",
    "X_train = vectorizer.fit_transform(train_reviews)\n",
    "X_test = vectorizer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 25000), (5000, 25000))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000,), (5000,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_sentiments.ravel()\n",
    "y_test = test_sentiments.ravel()\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TF- IDF LOGISTIC REGRESSION ###\n",
    "\n",
    "model_1 = LogisticRegression()\n",
    "model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8926"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prédiction et évaluation\n",
    "\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "\n",
    "score_1 = accuracy_score(y_test, y_pred_1)\n",
    "score_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And you should get about 90% accuracy this time. Other classic but more sophisticated features include N-grams, part-of-speech tagging and syntax trees, you can read more about these there:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/\n",
    "\n",
    "But we will stop there for the classic approaches and go to deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...unless you are ahead of time, in this case learn about Bags of N-grams by yourself, and try them out :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks : Long Short Term Memory networks (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already covered feed-forward neural networks during the computer vision and the recommended system module. For natural language processing, one type of popular deep learning architecture is called Reccurent Neural Networks (RNNs). RNNs differ from feed-forward networks in the sense that some of their inner layers are recursively updated while iterating over the sequence of words given in input. We are going to use one specific RNN architecture called Long-Short Term Memory networks (LSTMs), which have been especially successful in various NLP tasks, including automatic translation, question answering, ... and text classification, our case study in this module.\n",
    "\n",
    "Learn more about how RNNs and LSTMs encode texts as vectors first:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "If you want to understand in depth how one LSTM cell is working, you can go through these two articles:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using sequential models such as LSTMs, stopwords, punctuation and words suffixes carry semantics, and have thus much more importance than when using BOW-based models. Hence with these models we will only remove html tags, and keep all these :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va donner en entrée du modèle les reviews entières. Elles seront converties en leur indice i dans le dico\n",
    "# des occurences des mots. Cette indice correspondra à la ligne ei dans la matrice E des embeddings.\n",
    "\n",
    "# Pour chaque review, le RNN fait du feed-forward pour attribuer des valeurs aux embeddings ei de chaque mot de la\n",
    "# review. Ensuite, il compare le résultat y_pred au y_true et corrige les coefficients des ei en faisant du \n",
    "# back-propagation pour que le résultat match le plus possible avec y_true.\n",
    "\n",
    "# Les ei sont ensuite corrigées en fonction des mots des autres reviews, et le RNN repasse plusieurs fois sur le\n",
    "# dataset (dépend du nombre d'epoch fixé).\n",
    "\n",
    "# Les valeurs des coefs des ei ne sont pas réinitialisés entre les phrases des reviews. Donc plus un mot revient\n",
    "# dans les reviews, plus ses coefs sont corrigés.\n",
    "\n",
    "imdb_deep_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "                           special_chars = False, lowercase = True, stemming = False, \n",
    "                           stopwords = False, list_output = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there s a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei s love in the time of money is a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably my all time favorite movie a story of...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this show was an amazing fresh innovative idea...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>if you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production the filming tech...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there s a family where a little boy ...  negative\n",
       "4  petter mattei s love in the time of money is a...  positive\n",
       "5  probably my all time favorite movie a story of...  positive\n",
       "6  i sure would like to see a resurrection of a u...  positive\n",
       "7  this show was an amazing fresh innovative idea...  negative\n",
       "8  encouraged by the positive comments about this...  negative\n",
       "9  if you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_deep_clean_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping a validation set for early stopping is a good habit when training deep models, so let's resplit the dataset, and save the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean = imdb_deep_clean_dataset.iloc[:40000]\n",
    "valid_deep_clean = imdb_deep_clean_dataset.iloc[40000:45000]\n",
    "test_deep_clean = imdb_deep_clean_dataset.iloc[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on crée un dossier imdb_clean s'il n'existe pas\n",
    "\n",
    "outdir = '../data/imdb_clean/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée des csv pour split les données\n",
    "\n",
    "train_deep_clean.to_csv(outdir + 'train.csv', index = False)\n",
    "valid_deep_clean.to_csv(outdir + 'valid.csv', index = False)\n",
    "test_deep_clean.to_csv(outdir + 'test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can restart the code from there in case of a crash :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../data/imdb_clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean = pd.read_csv(outdir + 'train.csv')\n",
    "valid_deep_clean = pd.read_csv(outdir + 'valid.csv')\n",
    "test_deep_clean = pd.read_csv(outdir + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LSTMs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "#Some lines that allow for faster training with this version of tensorflow for these models\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Keras to implement an LSTM network. First we need to encode all the reviews as a list of indexes, where each word is replaced by its embedding index using keras \"Tokenizer\". To make all training reviews the same size, we will make them the same size as the longest review and add the special token < pad > as many time as necessary to all the other ones with the function `pad_sequences`. This token will be ignored by the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 25000 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, split=' ', oov_token='<unw>', filters=' ') # création instance + params\n",
    "tokenizer.fit_on_texts(pd.concat([train_deep_clean,valid_deep_clean]).review) # fit sur train et valid sets\n",
    "\n",
    "# This encodes our sentence as a sequence of integer\n",
    "# each integer being the index of each word in the vocabulary\n",
    "train_seqs = tokenizer.texts_to_sequences(train_deep_clean.review)\n",
    "valid_seqs = tokenizer.texts_to_sequences(valid_deep_clean.review)\n",
    "test_seqs = tokenizer.texts_to_sequences(test_deep_clean.review)\n",
    "\n",
    "# We need to pad the sequences so that they are all the same length :\n",
    "# the length of the longest one\n",
    "max_seq_length = max( [len(seq) for seq in train_seqs + valid_seqs] )\n",
    "\n",
    "X_train = pad_sequences(train_seqs, max_seq_length)\n",
    "X_valid = pad_sequences(valid_seqs, max_seq_length)\n",
    "X_test = pad_sequences(test_seqs, max_seq_length)\n",
    "\n",
    "# on garde que la 2e colonne de l'output de get_dummies car elle comporte des 0 et des 1\n",
    "y_train = pd.get_dummies(train_deep_clean.sentiment).values[:,1]\n",
    "y_valid = pd.get_dummies(valid_deep_clean.sentiment).values[:,1]\n",
    "y_test = pd.get_dummies(test_deep_clean.sentiment).values[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill the following function to implement a simple LSTM model : one embedding layer, one LSTM layer, and a final dense layer that yields a single score with a sigmoid activation function. Use Keras' Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(vocab_size, embedding_dim, seq_length, lstm_out_dim):\n",
    "    #TOFILL\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length=seq_length))\n",
    "    model.add(LSTM(lstm_out_dim))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='SGD',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2505, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 2,741,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200  #Bigger than embedding dim, as it combines all the words of each review\n",
    "\n",
    "model = get_lstm_model(max_vocab_size, embedding_dim, max_seq_length, lstm_out_dim)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 212s 339ms/step - loss: 0.6930 - accuracy: 0.5112 - val_loss: 0.6928 - val_accuracy: 0.5238\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 212s 339ms/step - loss: 0.6928 - accuracy: 0.5178 - val_loss: 0.6929 - val_accuracy: 0.5038\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 51.40%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty low accuracy isn't it ? Actually it is very easy to incorrectly train a deep neural net. Change the optimizer with \"adam\" instead of \"SGD\", add a dropout layer after the LSTM layer for regularization, and use early stopping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout, early stopping, adam\n",
    "def get_lstm_model_2(vocab_size, embedding_dim, seq_length, lstm_out_dim, dropout_rate):\n",
    "    #TOFILL    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length=seq_length))\n",
    "    model.add(LSTM(lstm_out_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2505, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 2,741,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_lstm_model_2(max_vocab_size, embedding_dim, max_seq_length, lstm_out_dim, dropout_rate)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 201s 321ms/step - loss: 0.4339 - accuracy: 0.8008 - val_loss: 0.5354 - val_accuracy: 0.7134\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 201s 321ms/step - loss: 0.2994 - accuracy: 0.8766 - val_loss: 0.3519 - val_accuracy: 0.8406\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 201s 321ms/step - loss: 0.1960 - accuracy: 0.9265 - val_loss: 0.2770 - val_accuracy: 0.8878\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 201s 321ms/step - loss: 0.1445 - accuracy: 0.9491 - val_loss: 0.3367 - val_accuracy: 0.8818\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 201s 321ms/step - loss: 0.1053 - accuracy: 0.9637 - val_loss: 0.3641 - val_accuracy: 0.8868\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 89.00%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. If we'd run for a longer time, we'd get a bit better results from our classic methods, but that's still quite slow for little improvement. We could also grid search for all hyper parameters (embedding and layer sizes, dropout rate, ...), but that's not the goal today, remember however that grid-search is standard when optimizing a model predictive performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for arbitrary sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try predict the sentiment of any kind of sentence in english, try your own. You first need to encode each review as a sequence of indexes (called tokens in keras), to pad these sequances, and finally predict the score with your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really liked the movie and had fun \n",
      " [[0.9235051]]\n",
      "quite good but a bit long \n",
      " [[0.87196994]]\n",
      "worst movie on the planet , so boring \n",
      " [[0.00225611]]\n"
     ]
    }
   ],
   "source": [
    "good = \"i really liked the movie and had fun\"\n",
    "less_good = \"quite good but a bit long\"\n",
    "bad = \"worst movie on the planet , so boring\"\n",
    "for review in [good,less_good,bad]:\n",
    "    #TOFILL\n",
    "    encoded_review = tokenizer.texts_to_sequences([review])\n",
    "    encoded_padded_review = pad_sequences(encoded_review, max_seq_length)\n",
    "    print(review,\"\\n\",model.predict(encoded_padded_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize embeddings with pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of LSTMs is a bit heavy, one way to speed this up is to re-use pre-trained word embeddings. Many such embeddings are available on the net. Read this to understand how are produced word embeddings and why they encode information that helps with all NLP tasks:\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use GloVe embeddings, download and load the embeddings produced from 6 billions documents from : https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../data/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97728 unique words in vocabulary\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('%s unique words in vocabulary' % len(word_index))\n",
    "\n",
    "# Les embeddings importés de GloVe décrivent 400 000 mots. Notre word_index lui contient 97000 mots.\n",
    "# On va donc assigner à nos mots les embeddings de GloVe (s'ils existent dans GloVe - sinon on crée un embedding\n",
    "# avec des valeurs aléatoire pour le mot qui n'y est pas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our word index, search for each of our 25000 most frequents words if they exist in the pretrained GloVe embeddings and assign them to their corresponding row index in the embedding matrix. If they don't exist in the GloVe embeddings, assign a random vector :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_index_sliced = dict(itertools.islice(word_index.items(), 25000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# Allocate the embeddings matrix\n",
    "embedding_matrix = np.zeros((max_vocab_size, embedding_dim))\n",
    "\n",
    "\n",
    "for word, i in word_index_sliced.items(): # on prend tous les mots entre 0 et 25000 du word_index\n",
    "    #TOFILL\n",
    "    if word in embeddings_index.keys():\n",
    "        embedding_matrix[i-1] = embeddings_index[word] # i commence à 1 donc on fait i-1\n",
    "    else :\n",
    "        embedding_matrix[i-1] = np.random.rand(embedding_dim) # on génère aléatoirement le vecteur de (100,) du mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change your LSTM model so that the embedding layer is initialized with the pretrained embeddings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix):\n",
    "    #TOFILL\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length=seq_length, weights=[embedding_matrix]))\n",
    "    model.add(LSTM(lstm_out_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# On rajoute en argument de Embedding l'argument weights et on met la matrice entre crochet pour rajouter des \n",
    "# embeddings pré-entrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2505, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 2,741,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_lstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 203s 325ms/step - loss: 0.5641 - accuracy: 0.6903 - val_loss: 0.3774 - val_accuracy: 0.8444\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 203s 325ms/step - loss: 0.2790 - accuracy: 0.8889 - val_loss: 0.2678 - val_accuracy: 0.8954\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 203s 325ms/step - loss: 0.1692 - accuracy: 0.9383 - val_loss: 0.2667 - val_accuracy: 0.8978\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 203s 325ms/step - loss: 0.1062 - accuracy: 0.9637 - val_loss: 0.3236 - val_accuracy: 0.8934\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 203s 325ms/step - loss: 0.0619 - accuracy: 0.9805 - val_loss: 0.3452 - val_accuracy: 0.8948\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 89.00%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the validation accuracy indeed progressed much faster than previously.\n",
    "\n",
    "For more speed-up, at the price of accuracy, let's fix the embeddings so that they are not trainable parameters of the model, meaning they won't be updated during training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                    trainable_embeddings):\n",
    "    #TOFILL\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length=seq_length, weights=[embedding_matrix], trainable = False))\n",
    "    model.add(LSTM(lstm_out_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Dans la couche Embedding, on fixe trainable = False pour que les vecteurs des embeddings soient pas MAJ\n",
    "# pendant l'entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 2505, 100)         2500000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,741,001\n",
      "Trainable params: 241,001\n",
      "Non-trainable params: 2,500,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "trainable_embeddings = False\n",
    "\n",
    "model = get_lstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, trainable_embeddings)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the change in the number of trainable parameters in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 181s 289ms/step - loss: 0.6656 - accuracy: 0.5735 - val_loss: 0.6505 - val_accuracy: 0.5862\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 181s 289ms/step - loss: 0.6025 - accuracy: 0.6536 - val_loss: 0.5520 - val_accuracy: 0.7088\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 179s 287ms/step - loss: 0.5098 - accuracy: 0.7415 - val_loss: 0.4682 - val_accuracy: 0.7732\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 180s 288ms/step - loss: 0.4147 - accuracy: 0.8071 - val_loss: 0.3953 - val_accuracy: 0.8250\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 180s 287ms/step - loss: 0.3441 - accuracy: 0.8480 - val_loss: 0.3619 - val_accuracy: 0.8420\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 84.66%\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By fixing the word embeddings, the training time shrunk a bit, but the validation accuracy is progressing more slowly and reaching a limit. Depending on the network architecture, the trade-off can be interesting, here not so much, just know this is a possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional and stacked LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs parse the text from left to right, but doing it also from right to left and concatening the two output vectors improved the results. These are called bidirectional LSTMs. It is also possible to stack multiple LSTM layers.\n",
    "\n",
    "This image is a good illustration of how these two variants work:\n",
    "\n",
    "https://www.researchgate.net/figure/Illustrations-for-basic-LSTMs-and-the-three-layer-stacked-LSTM-model-for-the-sequential_fig3_313115860\n",
    "\n",
    "\n",
    "First modify your network to make a bidirectional LSTM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                     lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                    trainable_embeddings):\n",
    "    #TOFILL\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_bilstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try stacking multiple bidirectional LSTM layers, where the number of layers `n_layers` is a parameter of the function building the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilayer_bilstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                                lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                                trainable_embeddings, n_layers):\n",
    "    #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 100\n",
    "dropout_rate = 0.2\n",
    "n_layers = 2\n",
    "\n",
    "model = get_multilayer_bilstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, True, n_layers)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the max accuracy reached is not much better than our TF-IDF model. This happens because the full word order is actually not so important for Sentiment Analysis. For this task, Convolutional Neural Networks can attain comparable performances faster, as they have simpler architectures. But that's not true for other task such as translation, question answering, ... (which are tasks that are a bit too long to train to be included in this course, hence the choice of sentiment an analysis to practice RNNs).\n",
    "\n",
    "Let's do it with a convolutional model by using 1D convolution with a kernel size of 3 over the word embeddings (this means that it will convolve the embeddings of the consecutive words 3 by 3), followed by a 1D max pooling and a dense ReLU layer before the final sigmoid :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                                out_dim, dropout_rate, embedding_matrix,\n",
    "                                                trainable_embeddings):\n",
    "    #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_conv_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       out_dim, dropout_rate, embedding_matrix, True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very optional parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parts are meant to be resources to explore if you are interested in the advanced concept of attention in deep nets. There are explanation links, as well as links with code for each of them, but don't feel obliged to implement all of them, these are meant to help understanding each of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is a mechanism that changes the output of an LSTM : instead of outputting the final hidden state vector $h_n$ where $n$ is the length of the encoded text, attention plugs on top of a LSTM and returns a combination of all the hidden state vectors at each word position $\\ \\sum_{t=1}^n \\alpha_t h_t$ (where $\\alpha_t \\in (0,1))$, and thus allows to pay a different attention to each part of the text, hence the name. \n",
    "\n",
    "It has been originally proposed for sequence to sequence models, like translation models, where there is a different attention combination computed for each translated output word. It is thus less useful for text classification, but it can be adapted, by computing a single output combination of all the hidden states, as explained in Section 3.3 of the following article :\n",
    "\n",
    "https://www.aclweb.org/anthology/P16-2034.pdf\n",
    "\n",
    "Here is a link about how to apply attention for text classification with Keras:\n",
    "\n",
    "https://www.kaggle.com/yshubham/simple-lstm-for-text-classification-with-attention\n",
    "\n",
    "\n",
    "You can also read the following link to understand how attention works in sequence to sequence models, which are nothing more than a reversed LSTM (the decoder) on top of a first LSTM (the encoder), in this case for translation where it helps aligning words in two different languages :\n",
    "\n",
    "https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer architecture for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State of the art models in NLP are not RNNs anymore, but Transformers. Transformers do not read text sequentially like RNNs, the core concept of Transformers is self-attention, an attention mechanism that combine separately each word embedding with the other word embeddings of the text. There are multiple such attention mechanisms called \"attention heads\" in a layer, and multiple such layers are stacked.\n",
    "\n",
    "Read this article to understand the self-attention layer:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "\n",
    "This article explains very well the Transformer for sequence to sequence models (again remember that a text classification model is just the encoder part of a sequence to sequence model) :\n",
    "\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Keras code to do text classification with a Transformer :\n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current state-of-the-art performance for text classification are achieved by doing transfer learning from the BERT model. The BERT model combines different techniques including the Transformer to pretrain in an unsupervised fashion on plain text. The last layers of BERT provide a high-level contextual representation of english sentences, and can then be reused in any NLP deep model.\n",
    "\n",
    "The BERT model : http://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "Keras application for text classification : https://pysnacks.com/bert-text-classification-with-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
